{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/00504/qsar_fish_toxicity.csv', sep=\";\" )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "dataset.columns = ['MLOGP','CIC0','GATS1i','NdssC','NdsCH','SM1_Dz','LC50']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [
    {
     "data": {
      "text/plain": "     MLOGP   CIC0  GATS1i  NdssC  NdsCH  SM1_Dz   LC50\n0    2.189  0.580   0.863      0      0   1.348  3.115\n1    2.125  0.638   0.831      0      0   1.348  3.531\n2    3.027  0.331   1.472      1      0   1.807  3.510\n3    2.094  0.827   0.860      0      0   1.886  5.390\n4    3.222  0.331   2.177      0      0   0.706  1.819\n..     ...    ...     ...    ...    ...     ...    ...\n902  2.801  0.728   2.226      0      2   0.736  3.109\n903  3.652  0.872   0.867      2      3   3.983  4.040\n904  3.763  0.916   0.878      0      6   2.918  4.818\n905  2.831  1.393   1.077      0      1   0.906  5.317\n906  4.057  1.032   1.183      1      3   4.754  8.201\n\n[907 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MLOGP</th>\n      <th>CIC0</th>\n      <th>GATS1i</th>\n      <th>NdssC</th>\n      <th>NdsCH</th>\n      <th>SM1_Dz</th>\n      <th>LC50</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.189</td>\n      <td>0.580</td>\n      <td>0.863</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.348</td>\n      <td>3.115</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2.125</td>\n      <td>0.638</td>\n      <td>0.831</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.348</td>\n      <td>3.531</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.027</td>\n      <td>0.331</td>\n      <td>1.472</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1.807</td>\n      <td>3.510</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.094</td>\n      <td>0.827</td>\n      <td>0.860</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.886</td>\n      <td>5.390</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3.222</td>\n      <td>0.331</td>\n      <td>2.177</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.706</td>\n      <td>1.819</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>902</th>\n      <td>2.801</td>\n      <td>0.728</td>\n      <td>2.226</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0.736</td>\n      <td>3.109</td>\n    </tr>\n    <tr>\n      <th>903</th>\n      <td>3.652</td>\n      <td>0.872</td>\n      <td>0.867</td>\n      <td>2</td>\n      <td>3</td>\n      <td>3.983</td>\n      <td>4.040</td>\n    </tr>\n    <tr>\n      <th>904</th>\n      <td>3.763</td>\n      <td>0.916</td>\n      <td>0.878</td>\n      <td>0</td>\n      <td>6</td>\n      <td>2.918</td>\n      <td>4.818</td>\n    </tr>\n    <tr>\n      <th>905</th>\n      <td>2.831</td>\n      <td>1.393</td>\n      <td>1.077</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.906</td>\n      <td>5.317</td>\n    </tr>\n    <tr>\n      <th>906</th>\n      <td>4.057</td>\n      <td>1.032</td>\n      <td>1.183</td>\n      <td>1</td>\n      <td>3</td>\n      <td>4.754</td>\n      <td>8.201</td>\n    </tr>\n  </tbody>\n</table>\n<p>907 rows Ã— 7 columns</p>\n</div>"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [
    {
     "data": {
      "text/plain": "           MLOGP      CIC0    GATS1i     NdssC     NdsCH    SM1_Dz      LC50\nMLOGP   1.000000 -0.235666  0.147205  0.121566  0.246403  0.464221  0.292005\nCIC0   -0.235666  1.000000 -0.146562 -0.141234  0.162922  0.200947  0.411057\nGATS1i  0.147205 -0.146562  1.000000 -0.010258  0.091836 -0.450535 -0.397964\nNdssC   0.121566 -0.141234 -0.010258  1.000000  0.188465  0.048438  0.171937\nNdsCH   0.246403  0.162922  0.091836  0.188465  1.000000  0.028810  0.172561\nSM1_Dz  0.464221  0.200947 -0.450535  0.048438  0.028810  1.000000  0.651652\nLC50    0.292005  0.411057 -0.397964  0.171937  0.172561  0.651652  1.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MLOGP</th>\n      <th>CIC0</th>\n      <th>GATS1i</th>\n      <th>NdssC</th>\n      <th>NdsCH</th>\n      <th>SM1_Dz</th>\n      <th>LC50</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>MLOGP</th>\n      <td>1.000000</td>\n      <td>-0.235666</td>\n      <td>0.147205</td>\n      <td>0.121566</td>\n      <td>0.246403</td>\n      <td>0.464221</td>\n      <td>0.292005</td>\n    </tr>\n    <tr>\n      <th>CIC0</th>\n      <td>-0.235666</td>\n      <td>1.000000</td>\n      <td>-0.146562</td>\n      <td>-0.141234</td>\n      <td>0.162922</td>\n      <td>0.200947</td>\n      <td>0.411057</td>\n    </tr>\n    <tr>\n      <th>GATS1i</th>\n      <td>0.147205</td>\n      <td>-0.146562</td>\n      <td>1.000000</td>\n      <td>-0.010258</td>\n      <td>0.091836</td>\n      <td>-0.450535</td>\n      <td>-0.397964</td>\n    </tr>\n    <tr>\n      <th>NdssC</th>\n      <td>0.121566</td>\n      <td>-0.141234</td>\n      <td>-0.010258</td>\n      <td>1.000000</td>\n      <td>0.188465</td>\n      <td>0.048438</td>\n      <td>0.171937</td>\n    </tr>\n    <tr>\n      <th>NdsCH</th>\n      <td>0.246403</td>\n      <td>0.162922</td>\n      <td>0.091836</td>\n      <td>0.188465</td>\n      <td>1.000000</td>\n      <td>0.028810</td>\n      <td>0.172561</td>\n    </tr>\n    <tr>\n      <th>SM1_Dz</th>\n      <td>0.464221</td>\n      <td>0.200947</td>\n      <td>-0.450535</td>\n      <td>0.048438</td>\n      <td>0.028810</td>\n      <td>1.000000</td>\n      <td>0.651652</td>\n    </tr>\n    <tr>\n      <th>LC50</th>\n      <td>0.292005</td>\n      <td>0.411057</td>\n      <td>-0.397964</td>\n      <td>0.171937</td>\n      <td>0.172561</td>\n      <td>0.651652</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.corr()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.        , 0.65165193],\n       [0.65165193, 1.        ]])"
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(dataset.SM1_Dz, dataset.LC50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "dataset['LC50'] = dataset['LC50'].astype('str')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "X = dataset['SM1_Dz'].values\n",
    "X = X[:, np.newaxis]\n",
    "y = dataset[['LC50']].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = dataset.iloc[:,:]\n",
    "X = X[:, np.newaxis]\n",
    "y = dataset[['LC50']].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1.348],\n       [ 1.348],\n       [ 1.807],\n       [ 1.886],\n       [ 0.706],\n       [ 2.942],\n       [ 2.851],\n       [ 2.942],\n       [ 1.591],\n       [ 1.769],\n       [ 1.591],\n       [ 1.769],\n       [ 1.859],\n       [ 0.981],\n       [ 0.143],\n       [ 1.199],\n       [-0.115],\n       [ 1.107],\n       [ 1.807],\n       [-1.169],\n       [ 3.988],\n       [ 4.068],\n       [ 3.395],\n       [ 2.526],\n       [ 3.112],\n       [ 2.51 ],\n       [ 2.591],\n       [-0.937],\n       [ 1.748],\n       [ 0.024],\n       [ 2.837],\n       [-0.521],\n       [ 2.242],\n       [ 1.06 ],\n       [ 2.813],\n       [ 5.089],\n       [ 3.562],\n       [ 2.066],\n       [ 3.492],\n       [ 2.274],\n       [ 2.102],\n       [ 2.37 ],\n       [ 1.533],\n       [ 3.906],\n       [ 1.565],\n       [ 0.787],\n       [ 2.193],\n       [ 2.217],\n       [ 2.849],\n       [ 2.642],\n       [ 2.642],\n       [ 3.795],\n       [ 2.296],\n       [ 2.942],\n       [ 3.21 ],\n       [ 1.859],\n       [ 3.478],\n       [ 2.127],\n       [ 2.127],\n       [ 1.859],\n       [ 0.411],\n       [ 1.619],\n       [ 0.408],\n       [ 2.026],\n       [ 0.094],\n       [ 1.672],\n       [ 1.817],\n       [ 0.347],\n       [ 0.347],\n       [ 0.202],\n       [ 0.094],\n       [ 0.202],\n       [-1.053],\n       [ 0.202],\n       [ 0.202],\n       [ 0.202],\n       [-1.053],\n       [-1.306],\n       [-0.386],\n       [ 0.706],\n       [ 2.274],\n       [ 3.291],\n       [ 2.05 ],\n       [ 6.203],\n       [ 1.869],\n       [ 1.064],\n       [-1.96 ],\n       [ 1.492],\n       [ 0.479],\n       [ 1.442],\n       [ 1.587],\n       [ 2.942],\n       [ 1.859],\n       [ 3.21 ],\n       [ 2.127],\n       [ 1.859],\n       [ 0.893],\n       [ 0.057],\n       [ 4.063],\n       [ 3.045],\n       [ 2.608],\n       [ 0.846],\n       [ 2.876],\n       [ 1.195],\n       [ 1.05 ],\n       [ 1.506],\n       [ 0.846],\n       [ 2.248],\n       [-0.063],\n       [ 0.846],\n       [-0.063],\n       [ 2.06 ],\n       [ 0.996],\n       [ 0.996],\n       [ 2.604],\n       [ 2.419],\n       [ 0.8  ],\n       [ 0.548],\n       [-0.534],\n       [-0.786],\n       [-0.534],\n       [-0.128],\n       [ 0.8  ],\n       [ 0.156],\n       [ 0.408],\n       [ 0.156],\n       [ 3.291],\n       [ 1.795],\n       [-0.45 ],\n       [ 2.849],\n       [ 1.795],\n       [ 3.516],\n       [ 2.604],\n       [ 1.209],\n       [ 1.064],\n       [-0.226],\n       [-0.081],\n       [-0.081],\n       [ 3.124],\n       [ 0.468],\n       [-0.544],\n       [ 2.021],\n       [ 2.129],\n       [ 0.538],\n       [ 3.126],\n       [ 1.587],\n       [ 1.587],\n       [ 0.076],\n       [ 2.604],\n       [-0.917],\n       [-0.917],\n       [ 2.604],\n       [ 1.94 ],\n       [ 0.454],\n       [ 1.94 ],\n       [ 1.795],\n       [ 4.673],\n       [ 3.186],\n       [ 3.186],\n       [-0.129],\n       [ 1.466],\n       [ 3.289],\n       [ 3.953],\n       [ 3.503],\n       [-0.937],\n       [ 3.806],\n       [ 4.098],\n       [ 4.379],\n       [ 5.579],\n       [ 0.496],\n       [ 2.619],\n       [ 2.813],\n       [ 1.697],\n       [ 1.57 ],\n       [ 1.064],\n       [ 1.209],\n       [ 3.115],\n       [ 5.149],\n       [ 4.789],\n       [ 1.89 ],\n       [ 0.787],\n       [ 5.426],\n       [ 3.461],\n       [ 2.294],\n       [ 5.207],\n       [ 3.764],\n       [ 2.359],\n       [ 1.701],\n       [ 0.828],\n       [ 3.587],\n       [ 2.956],\n       [ 2.736],\n       [ 1.097],\n       [ 2.402],\n       [ 4.295],\n       [ 0.893],\n       [ 4.063],\n       [ 2.729],\n       [ 1.976],\n       [ 2.239],\n       [ 1.239],\n       [ 0.922],\n       [ 0.291],\n       [ 2.193],\n       [ 2.509],\n       [ 0.491],\n       [ 1.401],\n       [ 1.969],\n       [ 2.723],\n       [ 1.919],\n       [ 2.273],\n       [ 3.395],\n       [ 1.312],\n       [ 2.129],\n       [ 2.193],\n       [ 1.49 ],\n       [ 1.442],\n       [ 1.121],\n       [ 0.893],\n       [ 0.202],\n       [ 0.076],\n       [ 2.06 ],\n       [ 0.655],\n       [ 1.373],\n       [-0.473],\n       [ 0.434],\n       [ 4.379],\n       [-1.186],\n       [ 3.234],\n       [ 0.749],\n       [ 0.8  ],\n       [ 0.133],\n       [ 2.459],\n       [ 1.112],\n       [ 2.455],\n       [ 5.469],\n       [ 0.202],\n       [ 4.177],\n       [ 0.982],\n       [ 3.048],\n       [ 1.971],\n       [ 2.005],\n       [ 1.595],\n       [ 2.859],\n       [ 5.206],\n       [ 1.821],\n       [ 2.573],\n       [ 2.922],\n       [ 2.294],\n       [ 2.821],\n       [ 1.155],\n       [ 2.637],\n       [ 3.818],\n       [ 0.454],\n       [ 0.133],\n       [ 3.267],\n       [ 1.294],\n       [ 0.8  ],\n       [ 2.749],\n       [-0.521],\n       [ 0.888],\n       [ 1.121],\n       [ 2.314],\n       [ 2.545],\n       [ 1.742],\n       [-1.053],\n       [ 0.586],\n       [ 0.314],\n       [ 3.562],\n       [ 2.226],\n       [ 1.373],\n       [ 2.06 ],\n       [ 2.274],\n       [ 3.574],\n       [ 3.503],\n       [ 3.467],\n       [ 4.912],\n       [ 2.242],\n       [ 1.467],\n       [ 1.297],\n       [-0.059],\n       [ 2.715],\n       [ 2.042],\n       [ 1.433],\n       [ 1.845],\n       [ 1.795],\n       [ 3.315],\n       [ 2.786],\n       [ 1.769],\n       [ 0.706],\n       [ 3.105],\n       [ 1.246],\n       [ 1.246],\n       [ 1.58 ],\n       [ 1.882],\n       [ 4.038],\n       [ 0.602],\n       [ 1.459],\n       [ 1.587],\n       [ 4.207],\n       [ 3.182],\n       [ 0.158],\n       [ 4.833],\n       [ 1.209],\n       [ 2.813],\n       [ 3.659],\n       [-0.297],\n       [ 0.057],\n       [ 1.313],\n       [ 2.679],\n       [ 2.938],\n       [ 2.646],\n       [ 4.764],\n       [ 4.548],\n       [ 2.42 ],\n       [ 4.044],\n       [ 4.038],\n       [ 0.242],\n       [ 2.429],\n       [ 3.11 ],\n       [ 2.345],\n       [ 1.661],\n       [ 2.459],\n       [ 3.374],\n       [ 4.19 ],\n       [ 2.359],\n       [ 1.467],\n       [ 4.207],\n       [ 2.459],\n       [ 2.343],\n       [ 2.59 ],\n       [ 3.16 ],\n       [ 2.689],\n       [ 3.909],\n       [ 1.748],\n       [ 2.446],\n       [ 3.618],\n       [ 2.214],\n       [ 4.57 ],\n       [ 2.481],\n       [ 3.806],\n       [ 0.655],\n       [ 1.57 ],\n       [ 2.2  ],\n       [ 4.416],\n       [ 4.416],\n       [ 4.76 ],\n       [ 0.874],\n       [-2.03 ],\n       [ 3.311],\n       [ 1.064],\n       [ 1.514],\n       [ 1.437],\n       [ 2.297],\n       [ 2.031],\n       [ 3.225],\n       [ 3.291],\n       [ 0.915],\n       [ 1.579],\n       [ 1.182],\n       [ 3.259],\n       [ 3.358],\n       [ 4.332],\n       [ 4.23 ],\n       [ 2.056],\n       [ 1.121],\n       [-0.463],\n       [ 3.913],\n       [ 3.046],\n       [ 2.726],\n       [ 1.117],\n       [ 6.515],\n       [ 2.51 ],\n       [ 4.234],\n       [ 1.06 ],\n       [ 2.481],\n       [ 3.386],\n       [ 1.042],\n       [ 2.433],\n       [ 1.746],\n       [ 2.592],\n       [ 2.27 ],\n       [ 1.68 ],\n       [ 3.184],\n       [ 3.051],\n       [ 2.391],\n       [ 2.357],\n       [ 1.834],\n       [ 4.452],\n       [ 2.994],\n       [ 1.564],\n       [ 0.314],\n       [ 4.431],\n       [ 0.652],\n       [ 3.109],\n       [ 4.651],\n       [ 4.017],\n       [ 3.19 ],\n       [ 2.931],\n       [ 3.213],\n       [ 2.054],\n       [ 1.94 ],\n       [ 1.66 ],\n       [ 1.602],\n       [ 1.321],\n       [ 1.654],\n       [ 3.32 ],\n       [ 2.358],\n       [ 0.37 ],\n       [ 1.878],\n       [ 5.361],\n       [ 1.759],\n       [ 1.851],\n       [ 0.215],\n       [-0.165],\n       [ 1.064],\n       [ 5.176],\n       [ 3.896],\n       [ 1.395],\n       [ 2.646],\n       [ 4.203],\n       [ 1.654],\n       [ 2.433],\n       [ 3.522],\n       [ 2.527],\n       [ 3.842],\n       [ 4.066],\n       [ 3.167],\n       [-1.169],\n       [ 1.145],\n       [ 2.116],\n       [ 1.717],\n       [ 3.617],\n       [ 2.335],\n       [ 3.508],\n       [ 1.748],\n       [ 4.054],\n       [ 3.305],\n       [ 4.379],\n       [ 2.841],\n       [ 2.371],\n       [ 1.145],\n       [ 3.109],\n       [ 1.953],\n       [ 5.339],\n       [ 5.472],\n       [-1.265],\n       [ 1.188],\n       [ 2.57 ],\n       [ 2.799],\n       [ 3.571],\n       [ 3.046],\n       [ 1.795],\n       [ 3.557],\n       [ 2.974],\n       [ 2.773],\n       [-0.197],\n       [ 2.702],\n       [ 3.661],\n       [ 3.737],\n       [ 3.948],\n       [ 0.548],\n       [ 3.062],\n       [ 2.553],\n       [ 1.313],\n       [ 1.091],\n       [ 2.604],\n       [ 2.11 ],\n       [-0.316],\n       [ 2.206],\n       [ 1.002],\n       [ 2.841],\n       [ 2.841],\n       [ 2.296],\n       [ 2.502],\n       [ 2.357],\n       [ 1.619],\n       [ 2.502],\n       [ 4.639],\n       [ 4.738],\n       [ 1.442],\n       [ 0.787],\n       [ 3.617],\n       [ 1.586],\n       [ 1.701],\n       [ 3.259],\n       [ 1.801],\n       [ 1.548],\n       [ 2.42 ],\n       [-0.129],\n       [ 0.779],\n       [ 6.166],\n       [ 3.458],\n       [ 0.143],\n       [ 2.446],\n       [ 0.435],\n       [ 1.395],\n       [-0.703],\n       [ 0.113],\n       [ 3.026],\n       [ 2.352],\n       [ 4.639],\n       [ 0.489],\n       [ 0.226],\n       [ 4.579],\n       [ 1.748],\n       [ 2.51 ],\n       [ 1.886],\n       [ 2.102],\n       [ 2.102],\n       [ 2.052],\n       [ 1.7  ],\n       [ 2.37 ],\n       [ 1.748],\n       [ 1.996],\n       [ 3.854],\n       [ 2.604],\n       [ 1.643],\n       [ 1.373],\n       [ 1.587],\n       [ 3.478],\n       [ 2.081],\n       [ 3.291],\n       [ 3.291],\n       [ 2.529],\n       [ 0.562],\n       [ 1.294],\n       [ 2.492],\n       [ 0.575],\n       [ 1.94 ],\n       [ 1.75 ],\n       [ 1.61 ],\n       [ 2.217],\n       [ 2.729],\n       [ 1.737],\n       [ 1.61 ],\n       [ 4.206],\n       [ 2.64 ],\n       [-0.294],\n       [ 2.223],\n       [ 0.883],\n       [ 2.54 ],\n       [ 1.748],\n       [ 0.626],\n       [ 1.064],\n       [ 3.291],\n       [ 1.897],\n       [ 1.398],\n       [ 4.453],\n       [ 2.033],\n       [-0.241],\n       [ 1.569],\n       [ 0.757],\n       [-0.534],\n       [ 1.701],\n       [ 5.258],\n       [ 5.741],\n       [ 1.395],\n       [ 2.193],\n       [ 1.701],\n       [ 2.193],\n       [ 2.233],\n       [ 4.092],\n       [ 3.617],\n       [ 1.209],\n       [ 4.053],\n       [ 3.795],\n       [ 4.639],\n       [ 1.539],\n       [ 2.193],\n       [ 2.42 ],\n       [ 2.193],\n       [ 2.461],\n       [ 1.994],\n       [ 1.064],\n       [ 1.442],\n       [ 2.352],\n       [ 1.758],\n       [ 4.506],\n       [ 4.804],\n       [ 1.209],\n       [-1.358],\n       [ 2.242],\n       [ 0.8  ],\n       [ 2.904],\n       [ 1.94 ],\n       [ 2.239],\n       [ 1.701],\n       [ 2.239],\n       [ 3.112],\n       [ 2.576],\n       [ 4.594],\n       [ 4.926],\n       [ 4.19 ],\n       [ 2.239],\n       [ 1.395],\n       [ 2.65 ],\n       [ 1.239],\n       [ 1.06 ],\n       [ 2.461],\n       [ 2.573],\n       [ 1.748],\n       [ 1.515],\n       [ 4.234],\n       [ 3.11 ],\n       [ 2.239],\n       [ 1.395],\n       [ 2.239],\n       [ 1.61 ],\n       [ 1.968],\n       [ 1.61 ],\n       [ 1.229],\n       [ 1.506],\n       [-0.317],\n       [ 1.246],\n       [-0.41 ],\n       [ 3.237],\n       [ 2.048],\n       [ 3.315],\n       [ 1.06 ],\n       [-0.521],\n       [ 3.562],\n       [ 0.943],\n       [ 2.729],\n       [ 0.204],\n       [ 0.8  ],\n       [ 2.957],\n       [ 3.426],\n       [ 3.451],\n       [ 2.604],\n       [ 1.141],\n       [ 2.222],\n       [ 0.996],\n       [ 2.538],\n       [ 4.634],\n       [ 3.314],\n       [ 2.269],\n       [ 3.314],\n       [ 2.212],\n       [ 1.748],\n       [-0.172],\n       [ 2.278],\n       [ 2.51 ],\n       [ 3.318],\n       [ 1.272],\n       [ 1.237],\n       [ 3.377],\n       [ 2.463],\n       [ 2.688],\n       [ 1.442],\n       [ 0.68 ],\n       [ 4.179],\n       [ 0.501],\n       [ 3.474],\n       [ 2.974],\n       [ 0.347],\n       [ 0.202],\n       [-0.317],\n       [ 3.291],\n       [-0.386],\n       [-0.273],\n       [ 0.18 ],\n       [ 4.279],\n       [ 0.967],\n       [ 2.242],\n       [ 2.274],\n       [ 0.289],\n       [ 3.661],\n       [ 2.894],\n       [ 0.087],\n       [ 0.087],\n       [ 1.239],\n       [ 1.271],\n       [ 3.987],\n       [ 4.804],\n       [ 4.235],\n       [ 1.807],\n       [ 1.701],\n       [ 2.357],\n       [ 4.246],\n       [ 1.748],\n       [ 2.289],\n       [ 1.21 ],\n       [ 3.076],\n       [ 0.347],\n       [ 0.8  ],\n       [ 1.209],\n       [ 2.255],\n       [ 2.226],\n       [ 1.95 ],\n       [ 3.118],\n       [ 2.213],\n       [ 4.268],\n       [ 5.934],\n       [-0.022],\n       [ 3.909],\n       [ 1.548],\n       [ 4.098],\n       [ 1.02 ],\n       [ 4.907],\n       [ 1.795],\n       [-0.172],\n       [-0.317],\n       [-0.317],\n       [-0.564],\n       [ 1.672],\n       [ 0.202],\n       [-2.884],\n       [ 0.8  ],\n       [ 0.58 ],\n       [ 1.442],\n       [ 0.996],\n       [-0.792],\n       [ 2.957],\n       [ 0.996],\n       [ 5.124],\n       [ 2.459],\n       [ 1.971],\n       [ 0.655],\n       [ 3.038],\n       [ 2.502],\n       [ 1.074],\n       [ 3.558],\n       [-0.741],\n       [ 3.267],\n       [ 1.587],\n       [ 1.442],\n       [ 2.558],\n       [ 3.157],\n       [ 3.374],\n       [ 1.737],\n       [-0.45 ],\n       [ 2.437],\n       [ 1.947],\n       [ 1.974],\n       [ 0.8  ],\n       [ 2.226],\n       [-0.534],\n       [ 0.8  ],\n       [ 0.655],\n       [-0.534],\n       [ 3.089],\n       [ 2.226],\n       [ 2.081],\n       [-0.38 ],\n       [ 0.133],\n       [ 2.604],\n       [ 2.998],\n       [ 4.804],\n       [ 4.878],\n       [ 3.762],\n       [ 3.306],\n       [ 4.436],\n       [ 3.105],\n       [ 1.622],\n       [ 3.563],\n       [ 0.888],\n       [ 3.903],\n       [ 4.658],\n       [-0.406],\n       [ 0.052],\n       [ 2.129],\n       [ 3.358],\n       [ 2.899],\n       [ 3.958],\n       [ 2.152],\n       [ 1.936],\n       [ 2.782],\n       [ 2.59 ],\n       [ 4.633],\n       [ 2.577],\n       [ 3.618],\n       [ 3.618],\n       [ 2.209],\n       [ 4.091],\n       [ 3.018],\n       [ 2.859],\n       [ 0.43 ],\n       [-0.816],\n       [ 3.055],\n       [ 4.063],\n       [ 3.659],\n       [-2.089],\n       [-0.439],\n       [ 2.042],\n       [ 2.956],\n       [ 3.314],\n       [ 1.237],\n       [ 2.241],\n       [ 2.509],\n       [ 1.348],\n       [ 2.702],\n       [ 4.412],\n       [ 1.783],\n       [ 3.314],\n       [ 1.701],\n       [ 2.813],\n       [ 2.37 ],\n       [ 1.667],\n       [ 3.677],\n       [ 3.945],\n       [ 2.637],\n       [ 3.127],\n       [ 3.157],\n       [ 3.546],\n       [ 3.386],\n       [ 2.066],\n       [ 1.701],\n       [ 2.502],\n       [ 2.813],\n       [ 2.224],\n       [ 3.586],\n       [ 3.876],\n       [ 3.127],\n       [ 1.587],\n       [ 1.335],\n       [ 0.655],\n       [ 2.351],\n       [ 1.756],\n       [ 0.586],\n       [ 3.617],\n       [ 2.991],\n       [ 2.186],\n       [ 1.783],\n       [ 2.575],\n       [ 1.132],\n       [ 2.354],\n       [ 2.659],\n       [ 2.927],\n       [ 2.723],\n       [ 3.315],\n       [ 1.099],\n       [ 1.501],\n       [ 2.942],\n       [ 1.859],\n       [ 3.21 ],\n       [ 3.478],\n       [ 2.127],\n       [ 3.045],\n       [ 1.859],\n       [ 2.127],\n       [ 3.259],\n       [ 2.193],\n       [ 3.795],\n       [ 3.795],\n       [ 2.729],\n       [ 1.246],\n       [ 2.729],\n       [ 4.634],\n       [ 3.314],\n       [ 3.887],\n       [ 3.345],\n       [ 1.522],\n       [ 1.533],\n       [ 1.587],\n       [ 1.064],\n       [ 2.604],\n       [ 1.064],\n       [ 0.586],\n       [ 1.393],\n       [ 1.395],\n       [ 3.842],\n       [ 2.11 ],\n       [-0.145],\n       [ 0.736],\n       [ 2.621],\n       [ 2.813],\n       [ 3.259],\n       [ 2.102],\n       [ 2.102],\n       [ 1.887],\n       [ 1.391],\n       [ 1.49 ],\n       [ 2.241],\n       [ 2.005],\n       [ 1.701],\n       [ 1.701],\n       [ 1.61 ],\n       [ 1.886],\n       [ 3.267],\n       [ 2.51 ],\n       [ 2.241],\n       [ 0.43 ],\n       [-0.386],\n       [ 1.024],\n       [ 2.502],\n       [ 1.119],\n       [ 4.449],\n       [ 4.832],\n       [ 1.94 ],\n       [ 3.399],\n       [ 4.452],\n       [ 2.791],\n       [ 3.909],\n       [ 1.442],\n       [-1.174],\n       [ 1.655],\n       [ 3.247],\n       [ 1.798],\n       [ 1.317],\n       [ 2.288],\n       [ 3.148],\n       [ 0.736],\n       [ 3.983],\n       [ 2.918],\n       [ 0.906],\n       [ 4.754]])"
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "clf = LogisticRegression()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#X_train = LabelEncoder().fit_transform(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\reinaldopc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": "LogisticRegression()"
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "k_fold = KFold(n_splits=10, shuffle=False, random_state=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\reinaldopc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "c:\\users\\reinaldopc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "c:\\users\\reinaldopc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "c:\\users\\reinaldopc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "c:\\users\\reinaldopc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "c:\\users\\reinaldopc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "c:\\users\\reinaldopc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "c:\\users\\reinaldopc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "c:\\users\\reinaldopc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n",
      "c:\\users\\reinaldopc\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "result = cross_val_score(clf, X, y, cv=k_fold)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pontos do K-Fold:[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Pontos do K-Fold:\"+str(result))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"MÃ©dia Cross-Validation K-Fold: {0}\".format(result.mean()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}